{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d67a584-afe4-4972-a3cc-f257f5b3e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinRagAssist - Smart Investment Advisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1d4928-a3f6-4023-a995-094982c083d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 1 — Imports, config, load_data\n",
    "\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Paths\n",
    "DATA_CSV = r\"C:\\Users\\rahil\\Downloads\\cleaned_data_final.csv\"\n",
    "XGB_MODEL_PATH = r\"C:\\Users\\rahil\\Downloads\\xgb_risk_model.joblib\"\n",
    "\n",
    "# Final feature set (training uses rich features, UI uses grouped features)\n",
    "FEATURE_COLS = [\n",
    "    # Rich numeric features from CSV\n",
    "    \"Age\",\n",
    "    \"Income Level\",\n",
    "    \"Account Balance\",\n",
    "    \"Deposits\",\n",
    "    \"Withdrawals\",\n",
    "    \"Transfers\",\n",
    "    \"International Transfers\",\n",
    "    \"Investments\",\n",
    "    \"Loan Amount\",\n",
    "    \"Loan Term (Months)\",\n",
    "    \"Net Savings\",\n",
    "    \"Loan to Income Ratio\",\n",
    "    \"Investment Ratio\",\n",
    "\n",
    "    # Simplified features used in UI\n",
    "    \"AgeGroup\",\n",
    "    \"IncomeGroup\",\n",
    "    \"EmploymentStatus\",\n",
    "    \"LoanStatus\",\n",
    "    \"InvestmentGoal\",\n",
    "    \"InvestmentAmount\",\n",
    "]\n",
    "\n",
    "TARGET_COL = \"Risk Tolerance\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def load_data(path: str = DATA_CSV) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV into DataFrame.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Data file not found at {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55d642b-df78-4e9d-95aa-2a48d10f44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 2 — Stock summaries & comparison (compact)\n",
    "\n",
    "import numpy as _np\n",
    "\n",
    "def _cagr(s: pd.Series):\n",
    "    if s.empty: \n",
    "        return _np.nan\n",
    "    yrs = (s.index[-1] - s.index[0]).days / 365.25\n",
    "    return (s.iloc[-1] / s.iloc[0]) ** (1 / yrs) - 1 if yrs > 0 else _np.nan\n",
    "\n",
    "def _max_dd(s: pd.Series):\n",
    "    if s.empty: \n",
    "        return _np.nan\n",
    "    return ((s - s.cummax()) / s.cummax()).min()\n",
    "\n",
    "def summarize_stock_price_df(df: pd.DataFrame, price_col=\"Close\", name=\"STOCK\"):\n",
    "    if price_col not in df or df[price_col].dropna().empty:\n",
    "        return f\"### {name} — no price data.\"\n",
    "\n",
    "    s = df[price_col].dropna().sort_index()\n",
    "    r = s.pct_change().dropna()\n",
    "\n",
    "    return (\n",
    "        f\"### {name}\\n\"\n",
    "        f\"- Latest: {s.iloc[-1]:.2f}\\n\"\n",
    "        f\"- CAGR: {_cagr(s):.2%}\\n\"\n",
    "        f\"- Vol: {(r.std()*_np.sqrt(252)):.2%}\\n\"\n",
    "        f\"- Max DD: {_max_dd(s):.2%}\"\n",
    "    )\n",
    "\n",
    "def compare_two_price_series(df1, df2, price_col=\"Close\", name1=\"A\", name2=\"B\"):\n",
    "    \"\"\"\n",
    "    Clean, UI-friendly comparison of two assets.\n",
    "    Always returns readable Markdown (no raw dicts or dtype junk).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert Series safely\n",
    "    def safe_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def metrics(df):\n",
    "        if df is None or df.empty or price_col not in df.columns:\n",
    "            return {\"ok\": False}\n",
    "\n",
    "        s = df[price_col].dropna()\n",
    "        if s.empty:\n",
    "            return {\"ok\": False}\n",
    "\n",
    "        last = safe_float(s.iloc[-1])\n",
    "\n",
    "        # CAGR\n",
    "        try:\n",
    "            cagr = _cagr(s)\n",
    "            cagr = float(cagr) if cagr is not None else None\n",
    "        except:\n",
    "            cagr = None\n",
    "\n",
    "        # Volatility\n",
    "        daily = s.pct_change().dropna()\n",
    "        vol = float(daily.std() * (252 ** 0.5)) if not daily.empty else None\n",
    "\n",
    "        # Max drawdown\n",
    "        try:\n",
    "            maxdd = _max_dd(s)\n",
    "            maxdd = float(maxdd) if maxdd is not None else None\n",
    "        except:\n",
    "            maxdd = None\n",
    "\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"last\": last,\n",
    "            \"cagr\": cagr,\n",
    "            \"vol\": vol,\n",
    "            \"maxdd\": maxdd,\n",
    "        }\n",
    "\n",
    "    # Compute metrics\n",
    "    m1 = metrics(df1)\n",
    "    m2 = metrics(df2)\n",
    "\n",
    "    # Build Markdown\n",
    "    md = []\n",
    "    md.append(f\"## Comparison: {name1} vs {name2}\")\n",
    "\n",
    "    if not m1[\"ok\"] or not m2[\"ok\"]:\n",
    "        md.append(\"**Not enough price data to compare these two symbols.**\")\n",
    "        return \"\\n\".join(md)\n",
    "\n",
    "    # Summary table\n",
    "    md.append(\"### Snapshot\\n\")\n",
    "    md.append(f\"- **{name1}**: Price ₹{m1['last']:.2f}, CAGR {m1['cagr']*100:.2f}%, Vol {m1['vol']*100:.2f}%, MaxDD {m1['maxdd']*100:.2f}%\")\n",
    "    md.append(f\"- **{name2}**: Price ₹{m2['last']:.2f}, CAGR {m2['cagr']*100:.2f}%, Vol {m2['vol']*100:.2f}%, MaxDD {m2['maxdd']*100:.2f}%\")\n",
    "\n",
    "    # Pros & cons\n",
    "    md.append(\"\\n### Pros & Cons\\n\")\n",
    "\n",
    "    # Pros A\n",
    "    pros1 = []\n",
    "    if m1[\"cagr\"] > m2[\"cagr\"]:\n",
    "        pros1.append(\"Higher returns (CAGR).\")\n",
    "    if m1[\"vol\"] < m2[\"vol\"]:\n",
    "        pros1.append(\"Lower volatility.\")\n",
    "    if m1[\"maxdd\"] > m2[\"maxdd\"]:\n",
    "        pros1.append(\"Smaller drawdowns.\")\n",
    "\n",
    "    cons1 = []\n",
    "    if m1[\"cagr\"] < m2[\"cagr\"]:\n",
    "        cons1.append(\"Lower returns (CAGR).\")\n",
    "    if m1[\"vol\"] > m2[\"vol\"]:\n",
    "        cons1.append(\"Higher volatility.\")\n",
    "    if m1[\"maxdd\"] < m2[\"maxdd\"]:\n",
    "        cons1.append(\"Larger drawdowns.\")\n",
    "\n",
    "    # Pros B\n",
    "    pros2 = []\n",
    "    if m2[\"cagr\"] > m1[\"cagr\"]:\n",
    "        pros2.append(\"Higher returns (CAGR).\")\n",
    "    if m2[\"vol\"] < m1[\"vol\"]:\n",
    "        pros2.append(\"Lower volatility.\")\n",
    "    if m2[\"maxdd\"] > m1[\"maxdd\"]:\n",
    "        pros2.append(\"Smaller drawdowns.\")\n",
    "\n",
    "    cons2 = []\n",
    "    if m2[\"cagr\"] < m1[\"cagr\"]:\n",
    "        cons2.append(\"Lower returns (CAGR).\")\n",
    "    if m2[\"vol\"] > m1[\"vol\"]:\n",
    "        cons2.append(\"Higher volatility.\")\n",
    "    if m2[\"maxdd\"] < m1[\"maxdd\"]:\n",
    "        cons2.append(\"Larger drawdowns.\")\n",
    "\n",
    "    md.append(f\"#### {name1} Pros\\n\" + (\"\\n\".join(f\"- {x}\" for x in pros1) if pros1 else \"- None\"))\n",
    "    md.append(f\"#### {name1} Cons\\n\" + (\"\\n\".join(f\"- {x}\" for x in cons1) if cons1 else \"- None\"))\n",
    "\n",
    "    md.append(f\"\\n#### {name2} Pros\\n\" + (\"\\n\".join(f\"- {x}\" for x in pros2) if pros2 else \"- None\"))\n",
    "    md.append(f\"#### {name2} Cons\\n\" + (\"\\n\".join(f\"- {x}\" for x in cons2) if cons2 else \"- None\"))\n",
    "\n",
    "    # Verdict\n",
    "    score1 = (m1[\"cagr\"] > m2[\"cagr\"]) + (m1[\"vol\"] < m2[\"vol\"]) + (m1[\"maxdd\"] > m2[\"maxdd\"])\n",
    "    score2 = (m2[\"cagr\"] > m1[\"cagr\"]) + (m2[\"vol\"] < m1[\"vol\"]) + (m2[\"maxdd\"] > m1[\"maxdd\"])\n",
    "\n",
    "    verdict = name1 if score1 > score2 else name2 if score2 > score1 else \"Tie\"\n",
    "\n",
    "    md.append(f\"\\n### Final Verdict\\n**{verdict}** looks better overall for a conservative investor.\")\n",
    "\n",
    "    return \"\\n\".join(md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c092060-ad6f-45dc-8c43-aaf793e11ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Block 3 — XGBoost training + predict (UI-aligned)\n",
    "\n",
    "\n",
    "def _age_to_group(age):\n",
    "    try:\n",
    "        a = float(age)\n",
    "    except:\n",
    "        return \"26-35\"\n",
    "    if a < 26:\n",
    "        return \"18-25\"\n",
    "    elif a < 36:\n",
    "        return \"26-35\"\n",
    "    elif a < 46:\n",
    "        return \"36-45\"\n",
    "    elif a < 61:\n",
    "        return \"46-60\"\n",
    "    else:\n",
    "        return \"60+\"\n",
    "\n",
    "def _income_to_group(inc):\n",
    "    try:\n",
    "        v = float(inc)\n",
    "    except:\n",
    "        return \"30,000-70,000\"\n",
    "    if v < 30000:\n",
    "        return \"<30,000\"\n",
    "    elif v <= 70000:\n",
    "        return \"30,000-70,000\"\n",
    "    else:\n",
    "        return \"70,000+\"\n",
    "\n",
    "def _ensure_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add simplified/grouped features that UI uses.\"\"\"\n",
    "    d = df.copy()\n",
    "\n",
    "    if \"AgeGroup\" not in d:\n",
    "        d[\"AgeGroup\"] = d[\"Age\"].apply(_age_to_group) if \"Age\" in d else \"26-35\"\n",
    "\n",
    "    if \"IncomeGroup\" not in d:\n",
    "        d[\"IncomeGroup\"] = d[\"Income Level\"].apply(_income_to_group) if \"Income Level\" in d else \"30,000-70,000\"\n",
    "\n",
    "    d[\"EmploymentStatus\"] = d.get(\"Employment Status\", \"Salaried\").astype(str)\n",
    "    d[\"LoanStatus\"] = d.get(\"Loan Status\", \"No\").astype(str)\n",
    "\n",
    "    if \"InvestmentGoal\" not in d:\n",
    "        d[\"InvestmentGoal\"] = d.get(\"Investment Goals\", \"Growth\").astype(str)\n",
    "\n",
    "    if \"InvestmentAmount\" not in d:\n",
    "        if \"Investments\" in d:\n",
    "            d[\"InvestmentAmount\"] = d[\"Investments\"].fillna(0)\n",
    "        elif \"Net Savings\" in d:\n",
    "            d[\"InvestmentAmount\"] = d[\"Net Savings\"].fillna(0)\n",
    "        elif \"Account Balance\" in d:\n",
    "            d[\"InvestmentAmount\"] = d[\"Account Balance\"].fillna(0)\n",
    "        else:\n",
    "            d[\"InvestmentAmount\"] = 0.0\n",
    "\n",
    "    return d\n",
    "\n",
    "def preprocess(df: pd.DataFrame):\n",
    "    \"\"\"Prepare X: keep rich numeric features + encode grouped ones.\"\"\"\n",
    "    df = _ensure_features(df)\n",
    "\n",
    "    use_cols = [c for c in FEATURE_COLS if c in df.columns]\n",
    "    if not use_cols:\n",
    "        raise ValueError(\"None of the FEATURE_COLS found in dataset\")\n",
    "\n",
    "    X = df[use_cols].copy()\n",
    "\n",
    "    # Encode categoricals\n",
    "    cat_maps = {}\n",
    "    for c in X.select_dtypes(include=[\"object\"]).columns:\n",
    "        X[c] = X[c].fillna(\"missing\").astype(\"category\")\n",
    "        cat_maps[c] = list(X[c].cat.categories)\n",
    "        X[c] = X[c].cat.codes\n",
    "\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X.loc[:, :] = scaler.fit_transform(X.values)\n",
    "\n",
    "    return X, cat_maps, scaler\n",
    "\n",
    "def train_xgb_model(df: pd.DataFrame):\n",
    "    \"\"\"Train XGBoost and print accuracy.\"\"\"\n",
    "    if TARGET_COL not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain '{TARGET_COL}'\")\n",
    "\n",
    "    X, cat_maps, scaler = preprocess(df)\n",
    "    label_enc = LabelEncoder()\n",
    "    y = label_enc.fit_transform(df[TARGET_COL].astype(str))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=350,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        use_label_encoder=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"XGBoost test accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, preds, target_names=label_enc.classes_))\n",
    "\n",
    "    bundle = {\n",
    "        \"model\": model,\n",
    "        \"columns\": X.columns.tolist(),\n",
    "        \"cat_maps\": cat_maps,\n",
    "        \"scaler\": scaler,\n",
    "        \"label_encoder\": label_enc,\n",
    "    }\n",
    "    joblib.dump(bundle, XGB_MODEL_PATH)\n",
    "    print(f\"Saved model to {XGB_MODEL_PATH}\")\n",
    "    return bundle\n",
    "\n",
    "def load_xgb_model(path: str = XGB_MODEL_PATH):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\"Model not trained yet.\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_risk(bundle: Dict[str, Any], inp: Dict[str, Any]):\n",
    "    \"\"\"Predict risk tolerance for one user profile.\"\"\"\n",
    "    cols = bundle[\"columns\"]\n",
    "    cat_maps = bundle[\"cat_maps\"]\n",
    "    scaler = bundle[\"scaler\"]\n",
    "    le: LabelEncoder = bundle[\"label_encoder\"]\n",
    "    model = bundle[\"model\"]\n",
    "\n",
    "    row = {c: inp.get(c, \"missing\") for c in cols}\n",
    "    df_row = pd.DataFrame([row])\n",
    "\n",
    "    for col, cats in cat_maps.items():\n",
    "        val = df_row.at[0, col]\n",
    "        if val not in cats:\n",
    "            val = \"missing\" if \"missing\" in cats else cats[0]\n",
    "        df_row[col] = cats.index(val)\n",
    "\n",
    "    df_row = df_row.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "    df_row[cols] = scaler.transform(df_row[cols].values)\n",
    "\n",
    "    proba = model.predict_proba(df_row)[0]\n",
    "    idx = int(np.argmax(proba))\n",
    "\n",
    "    return {\n",
    "        \"prediction\": le.inverse_transform([idx])[0],\n",
    "        \"probability\": float(proba[idx]),\n",
    "        \"class_probabilities\": {label: float(p) for label, p in zip(le.classes_, proba)},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6aee58-6aab-48c9-bb4f-2c5c0b6bd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df = load_data()\n",
    "#     train_xgb_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f36c2c7-9a0a-449e-952b-54431d882ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rahil\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Block 4 — ChromaDB + embeddings setup\n",
    "\n",
    "\n",
    "# Required from Block 1 — add defaults here to avoid NameError\n",
    "try:\n",
    "    CHROMA_COLLECTION_NAME\n",
    "except NameError:\n",
    "    CHROMA_COLLECTION_NAME = \"fin_docs\"\n",
    "\n",
    "try:\n",
    "    CHROMA_DIR\n",
    "except NameError:\n",
    "    CHROMA_DIR = r\"C:\\Users\\rahil\\Downloads\\chroma_db\"\n",
    "\n",
    "try:\n",
    "    EMBEDDING_MODEL_NAME\n",
    "except NameError:\n",
    "    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "try:\n",
    "    DEFAULT_TOP_K\n",
    "except NameError:\n",
    "    DEFAULT_TOP_K = 4\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "def init_chroma(collection_name: str = CHROMA_COLLECTION_NAME, persist_dir: str = CHROMA_DIR):\n",
    "    \"\"\"\n",
    "    Initialize or load a ChromaDB collection.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    try:\n",
    "        coll = client.get_collection(name=collection_name)\n",
    "        print(f\"Loaded existing Chroma collection: {collection_name}\")\n",
    "    except Exception:\n",
    "        coll = client.create_collection(name=collection_name)\n",
    "        print(f\"Created new Chroma collection: {collection_name}\")\n",
    "\n",
    "    return client, coll\n",
    "\n",
    "\n",
    "def build_embeddings_and_upsert(docs, collection, embed_model_name: str = EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Encode text and insert into ChromaDB.\n",
    "    Each doc: {\"id\": \"...\", \"text\": \"...\", \"metadata\": {...}}\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(embed_model_name)\n",
    "\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    ids = [d[\"id\"] for d in docs]\n",
    "    metas = [d.get(\"metadata\", {}) for d in docs]\n",
    "\n",
    "    embs = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "    collection.upsert(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        metadatas=metas,\n",
    "        embeddings=embs.tolist(),\n",
    "    )\n",
    "\n",
    "    print(f\"Upserted {len(ids)} documents into collection '{collection.name}'\")\n",
    "\n",
    "\n",
    "def make_retriever(k: int = DEFAULT_TOP_K):\n",
    "    \"\"\"\n",
    "    Create retriever for similarity-based queries.\n",
    "    \"\"\"\n",
    "    hf = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "    \n",
    "    vect_store = Chroma(\n",
    "        collection_name=CHROMA_COLLECTION_NAME,\n",
    "        embedding_function=hf,\n",
    "        persist_directory=CHROMA_DIR,\n",
    "    )\n",
    "\n",
    "    return vect_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": k},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9188a0-aefa-4dbd-941c-5f2d4532e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 5 : RAG + yfinance + news fetch utilities (robust, defensive)\n",
    "\n",
    "from typing import List, Any\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Optional Tavily key (keep in .env if you have one)\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "\n",
    "def _normalize_ticker(q: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize user-provided ticker/name:\n",
    "    - Trim whitespace, convert to upper for tickers\n",
    "    - Remove common exchange suffixes like '.NS', '.BO' for NSE/BSE tickers\n",
    "    - If the input looks like a word (contains letters and spaces), return as-is for RAG searches\n",
    "    \"\"\"\n",
    "    if not isinstance(q, str) or not q.strip():\n",
    "        return q\n",
    "    q = q.strip()\n",
    "    # If purely alphabetic (or alnum) assume ticker and uppercase + strip suffixes\n",
    "    # e.g. \"TCS.NS\" -> \"TCS\"\n",
    "    if re.fullmatch(r\"[A-Za-z0-9\\.\\-]+\", q):\n",
    "        up = q.upper()\n",
    "        # drop known exchange suffixes like .NS .BO .NSE etc\n",
    "        up = re.sub(r\"(\\.NS|\\.BO|\\.NSE|\\.BSE)$\", \"\", up)\n",
    "        return up\n",
    "    return q\n",
    "\n",
    "\n",
    "def fetch_stock_data(ticker: str) -> Document | None:\n",
    "    \"\"\"\n",
    "    Fetch short stock summary from yfinance and wrap in a Document.\n",
    "    Returns None if no price/history available.\n",
    "    Robust to tickers with exchange suffixes (e.g. 'TCS.NS' -> 'TCS').\n",
    "    \"\"\"\n",
    "    if not ticker or not isinstance(ticker, str):\n",
    "        return None\n",
    "\n",
    "    q = _normalize_ticker(ticker)\n",
    "    # yfinance expects ticker like 'TCS.NS' for NSE — try a few fallbacks:\n",
    "    candidates = [q]\n",
    "    # if user provided plain ticker and we are in India, try adding .NS (best-effort)\n",
    "    if q.isalpha() and len(q) <= 5:\n",
    "        candidates.append(q + \".NS\")\n",
    "        candidates.append(q + \".BO\")\n",
    "\n",
    "    last_doc: Document | None = None\n",
    "    for cand in candidates:\n",
    "        try:\n",
    "            ticker_obj = yf.Ticker(cand)\n",
    "            hist = ticker_obj.history(period=\"1mo\")\n",
    "            if hist is None or hist.empty:\n",
    "                continue\n",
    "\n",
    "            last_close = float(hist[\"Close\"].iloc[-1])\n",
    "            avg_5 = float(hist[\"Close\"].tail(5).mean())\n",
    "\n",
    "            info = ticker_obj.info if isinstance(ticker_obj.info, dict) else {}\n",
    "            sector = info.get(\"sector\") or info.get(\"industry\") or \"N/A\"\n",
    "            industry = info.get(\"industry\") or \"N/A\"\n",
    "\n",
    "            text = (\n",
    "                f\"{cand} latest close: {last_close:.2f}. \"\n",
    "                f\"5-day avg close: {avg_5:.2f}. \"\n",
    "                f\"Sector: {sector}. \"\n",
    "                f\"Industry: {industry}.\"\n",
    "            )\n",
    "\n",
    "            last_doc = Document(page_content=text, metadata={\"source\": \"yfinance\", \"ticker\": cand})\n",
    "            break\n",
    "        except Exception:\n",
    "            # try next candidate, but don't crash\n",
    "            continue\n",
    "\n",
    "    return last_doc\n",
    "\n",
    "\n",
    "def fetch_news_tavily(query: str, max_results: int = 5, results: List[Any] | None = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Convert a list of news results (from any API) into a list of LangChain Documents.\n",
    "    If `results` is None this returns an empty list — caller should fetch live results where permitted.\n",
    "    Accepts dicts, objects with attributes, or plain strings.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        return []\n",
    "\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    # Debug info (safe)\n",
    "    if results:\n",
    "        first = results[0]\n",
    "        try:\n",
    "            info = {\"type\": type(first).__name__}\n",
    "            if isinstance(first, dict):\n",
    "                info[\"keys\"] = list(first.keys())\n",
    "            print(f\"[fetch_news_tavily] sample item info: {info}\")\n",
    "        except Exception:\n",
    "            print(\"[fetch_news_tavily] could not inspect first item\")\n",
    "\n",
    "    for i, item in enumerate(results[:max_results]):\n",
    "        try:\n",
    "            # dict with title + content\n",
    "            if isinstance(item, dict) and \"title\" in item and \"content\" in item:\n",
    "                title = str(item.get(\"title\", \"\")).strip()\n",
    "                content = str(item.get(\"content\", \"\")).strip()\n",
    "                text = f\"{title}: {content}\" if title else content\n",
    "\n",
    "            # dict with title + text/summary\n",
    "            elif isinstance(item, dict) and \"title\" in item and (\"text\" in item or \"summary\" in item):\n",
    "                title = str(item.get(\"title\", \"\")).strip()\n",
    "                content = str(item.get(\"text\", item.get(\"summary\", \"\"))).strip()\n",
    "                text = f\"{title}: {content}\" if title else content\n",
    "\n",
    "            # dict with text/summary only\n",
    "            elif isinstance(item, dict) and (\"text\" in item or \"summary\" in item):\n",
    "                text = str(item.get(\"text\", item.get(\"summary\", \"\"))).strip()\n",
    "\n",
    "            # object with attributes\n",
    "            elif hasattr(item, \"title\") or hasattr(item, \"content\") or hasattr(item, \"text\"):\n",
    "                title = getattr(item, \"title\", \"\") or \"\"\n",
    "                content = getattr(item, \"content\", getattr(item, \"text\", \"\")) or \"\"\n",
    "                title = str(title).strip()\n",
    "                content = str(content).strip()\n",
    "                text = f\"{title}: {content}\" if title else content\n",
    "\n",
    "            # plain string\n",
    "            elif isinstance(item, str):\n",
    "                text = item.strip()\n",
    "\n",
    "            # fallback\n",
    "            else:\n",
    "                text = str(item)\n",
    "\n",
    "            source = None\n",
    "            if isinstance(item, dict) and \"url\" in item:\n",
    "                source = item.get(\"url\")\n",
    "            elif hasattr(item, \"url\"):\n",
    "                source = getattr(item, \"url\")\n",
    "\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": source or \"unknown\", \"type\": \"news\", \"query\": query}))\n",
    "        except Exception as exc:\n",
    "            print(f\"[fetch_news_tavily] skipping result {i}: {exc}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def rag_query(query: str, retriever, collection, k: int | None = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Query the vector store (retriever). If results are missing or irrelevant,\n",
    "    fetch live data (yfinance + news placeholder), upsert into the collection,\n",
    "    and return the fresh documents.\n",
    "\n",
    "    Notes:\n",
    "    - retriever: LangChain retriever (expects get_relevant_documents(query))\n",
    "    - collection: Chroma collection used by build_embeddings_and_upsert()\n",
    "    - k: override for top-k (defaults to DEFAULT_TOP_K if available)\n",
    "    \"\"\"\n",
    "    if not isinstance(query, str) or not query.strip():\n",
    "        return []\n",
    "\n",
    "    # Determine k\n",
    "    if k is None:\n",
    "        k = globals().get(\"DEFAULT_TOP_K\", 4)\n",
    "\n",
    "    q_clean = query.strip()\n",
    "    q_norm = _normalize_ticker(q_clean)\n",
    "\n",
    "    # Try retrieval first (best-effort, defensive)\n",
    "    results: List[Document] = []\n",
    "    try:\n",
    "        if retriever is not None:\n",
    "            # some retrievers accept search_kwargs for k; try to pass it if available\n",
    "            try:\n",
    "                results = retriever.get_relevant_documents(q_clean)\n",
    "            except TypeError:\n",
    "                results = retriever.get_relevant_documents(q_clean, k=k)\n",
    "    except Exception as exc:\n",
    "        print(f\"[rag_query] retriever error: {exc}\")\n",
    "        results = []\n",
    "\n",
    "    # If results look relevant (contains query token in top doc), return them\n",
    "    if results:\n",
    "        try:\n",
    "            top = results[0]\n",
    "            top_text = (top.page_content or \"\").lower()\n",
    "            meta_text = str(top.metadata or \"\").lower()\n",
    "            # treat normalized ticker and original query as valid matches\n",
    "            if (q_clean.lower() in top_text) or (q_norm.lower() in top_text) or (q_clean.lower() in meta_text) or (q_norm.lower() in meta_text):\n",
    "                print(f\"[rag_query] found {len(results)} relevant docs for '{query}'\")\n",
    "                return results\n",
    "            else:\n",
    "                print(f\"[rag_query] retrieved docs exist but appear irrelevant for '{query}' (falling back to live fetch)\")\n",
    "        except Exception:\n",
    "            print(\"[rag_query] could not inspect retrieved docs; fetching live data\")\n",
    "\n",
    "    # Fallback: fetch live data (yfinance + news)\n",
    "    print(f\"[rag_query] fetching live data for '{query}' (yfinance + news placeholder)...\")\n",
    "    new_docs: List[Document] = []\n",
    "\n",
    "    # If the query looks like a ticker, attempt stock fetch\n",
    "    if (q_norm and q_norm.isalnum() and len(q_norm) <= 6) or q_clean.isupper():\n",
    "        sd = fetch_stock_data(q_clean)\n",
    "        if sd:\n",
    "            new_docs.append(sd)\n",
    "\n",
    "    # news fetch placeholder:\n",
    "    # Caller should call real news API (Tavily or other) and pass results to fetch_news_tavily.\n",
    "    # For safety here we do not call any external paid API — we expect the caller to pass `results` when available.\n",
    "    tavily_results = None\n",
    "    news_docs = fetch_news_tavily(q_clean, max_results=5, results=tavily_results)\n",
    "    new_docs.extend(news_docs)\n",
    "\n",
    "    # Upsert into DB if we have a helper available and collection provided\n",
    "    upsert_items = []\n",
    "    for i, d in enumerate(new_docs):\n",
    "        upsert_items.append({\"id\": f\"live_{i}_{q_norm}\", \"text\": d.page_content, \"metadata\": d.metadata})\n",
    "\n",
    "    if upsert_items and collection is not None and \"build_embeddings_and_upsert\" in globals():\n",
    "        try:\n",
    "            build_embeddings_and_upsert(upsert_items, collection)\n",
    "        except Exception as exc:\n",
    "            print(f\"[rag_query] upsert failed: {exc}\")\n",
    "\n",
    "    return new_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4c60421-67f2-466f-9723-834834fdf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Tools for the agent\n",
    "\n",
    "from langchain.agents import Tool\n",
    "\n",
    "\n",
    "# Risk profiling tool\n",
    "def risk_tool_func(user_input: dict):\n",
    "    \"\"\"\n",
    "    Predict the user's risk tolerance using the stored XGBoost model.\n",
    "    Expects keys like AgeGroup, IncomeGroup, EmploymentStatus, LoanStatus,\n",
    "    InvestmentGoal, InvestmentAmount (aligned with predict_risk).\n",
    "    \"\"\"\n",
    "    bundle = load_xgb_model()\n",
    "    return predict_risk(bundle, user_input)\n",
    "\n",
    "\n",
    "risk_tool = Tool(\n",
    "    name=\"RiskProfiler\",\n",
    "    func=risk_tool_func,\n",
    "    description=(\n",
    "        \"Predicts a user's risk tolerance (High, Medium, Low) based on \"\n",
    "        \"their financial profile (age group, income group, employment, loans, goals, etc.).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Market data / RAG tool\n",
    "def market_tool_func(query: str):\n",
    "    \"\"\"\n",
    "    Returns market or stock-related context from the RAG setup.\n",
    "    \"\"\"\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "    docs = rag_query(query, retriever, coll) or []\n",
    "    if not docs:\n",
    "        return \"No market information found.\"\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "market_tool = Tool(\n",
    "    name=\"MarketData\",\n",
    "    func=market_tool_func,\n",
    "    description=(\n",
    "        \"Provides market/stock context from the internal RAG store. \"\n",
    "        \"Accepts a stock ticker (e.g. AAPL) or company name (e.g. Apple).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Stock comparison tool\n",
    "def compare_tool_func(stock1: str, stock2: str):\n",
    "    \"\"\"\n",
    "    Fetch RAG summaries for two stocks and return a combined view\n",
    "    for the LLM to analyse further (pros/cons, verdict).\n",
    "    \"\"\"\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "\n",
    "    docs1 = rag_query(stock1, retriever, coll) or []\n",
    "    docs2 = rag_query(stock2, retriever, coll) or []\n",
    "\n",
    "    summary1 = \"\\n\\n\".join(d.page_content for d in docs1) or \"No data found.\"\n",
    "    summary2 = \"\\n\\n\".join(d.page_content for d in docs2) or \"No data found.\"\n",
    "\n",
    "    return (\n",
    "        f\"Stock 1: {stock1}\\n{summary1}\\n\\n\"\n",
    "        f\"Stock 2: {stock2}\\n{summary2}\\n\\n\"\n",
    "        \"Now compare them with pros/cons and a final verdict.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _compare_wrapper(pair: str):\n",
    "    # handle 'AAPL,MSFT' or 'AAPL, MSFT'\n",
    "    parts = [p.strip() for p in pair.split(\",\")]\n",
    "    if len(parts) != 2:\n",
    "        return \"Please provide two tickers as 'TICKER1,TICKER2'.\"\n",
    "    return compare_tool_func(parts[0], parts[1])\n",
    "\n",
    "\n",
    "compare_tool = Tool(\n",
    "    name=\"CompareStocks\",\n",
    "    func=_compare_wrapper,\n",
    "    description=\"Compare two stocks by providing input as 'TICKER1,TICKER2'.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c79fe285-2fa7-4f11-8b2f-0086b0c6b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Agent setup with LangChain\n",
    "\n",
    "import re\n",
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def parse_user_profile(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse simple 'key=value' pairs (comma-separated) into a dictionary.\n",
    "\n",
    "    Example:\n",
    "        \"Age=30, Occupation=Engineer, Income=60000\"\n",
    "    \"\"\"\n",
    "    profile = {}\n",
    "    for part in text.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part or \"=\" not in part:\n",
    "            continue\n",
    "        key, val = part.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        val = val.strip()\n",
    "        # Try to convert numeric values\n",
    "        try:\n",
    "            val = float(val) if \".\" in val else int(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        profile[key] = val\n",
    "    return profile\n",
    "\n",
    "\n",
    "# 1) Risk profiling tool\n",
    "def risk_tool_func(user_input: str):\n",
    "    \"\"\"\n",
    "    Accepts text like:\n",
    "      \"Age=30, Income=60000, Employment Status=Salaried, Loan Amount=10000, Investment Goals=Growth, Investments=25000\"\n",
    "    and maps it into the simplified feature space expected by predict_risk.\n",
    "    \"\"\"\n",
    "    bundle = load_xgb_model()\n",
    "    raw = parse_user_profile(user_input)\n",
    "\n",
    "    # Map raw values into grouped features used by the model\n",
    "    age_val = raw.get(\"Age\", raw.get(\"age\", 30))\n",
    "    inc_val = raw.get(\"Income\", raw.get(\"Income Level\", raw.get(\"income\", 60000)))\n",
    "\n",
    "    age_group = _age_to_group(age_val)\n",
    "    income_group = _income_to_group(inc_val)\n",
    "\n",
    "    emp = raw.get(\"Employment Status\", raw.get(\"EmploymentStatus\", \"Salaried\"))\n",
    "    loan_status_raw = str(raw.get(\"Loan Status\", raw.get(\"LoanStatus\", \"\"))).lower()\n",
    "    loan_amt = float(raw.get(\"Loan Amount\", raw.get(\"loan_amount\", 0)) or 0)\n",
    "    has_loan = loan_amt > 0 or loan_status_raw in [\"yes\", \"y\", \"true\", \"1\"]\n",
    "    loan_status = \"Yes\" if has_loan else \"No\"\n",
    "\n",
    "    goal = raw.get(\"Investment Goals\", raw.get(\"Goal\", raw.get(\"InvestmentGoal\", \"Growth\")))\n",
    "    invest_amt = raw.get(\n",
    "        \"InvestmentAmount\",\n",
    "        raw.get(\"Investments\", raw.get(\"Net Savings\", raw.get(\"Account Balance\", 0.0))),\n",
    "    )\n",
    "    try:\n",
    "        invest_amt = float(invest_amt)\n",
    "    except Exception:\n",
    "        invest_amt = 0.0\n",
    "\n",
    "    feature_row = {\n",
    "        \"AgeGroup\": age_group,\n",
    "        \"IncomeGroup\": income_group,\n",
    "        \"EmploymentStatus\": str(emp),\n",
    "        \"LoanStatus\": loan_status,\n",
    "        \"InvestmentGoal\": str(goal),\n",
    "        \"InvestmentAmount\": invest_amt,\n",
    "    }\n",
    "\n",
    "    return predict_risk(bundle, feature_row)\n",
    "\n",
    "\n",
    "risk_tool = Tool(\n",
    "    name=\"RiskProfiler\",\n",
    "    func=risk_tool_func,\n",
    "    description=(\n",
    "        \"Predicts a user's risk tolerance (High, Medium, Low) from profile data. \"\n",
    "        \"Input format: key=value pairs separated by commas. \"\n",
    "        \"Example: 'Age=30, Income=60000, Employment Status=Salaried, \"\n",
    "        \"Loan Amount=10000, Investment Goals=Growth, Investments=25000'.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Market data tool (RAG over your vector store)\n",
    "def market_tool_func(query: str):\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "    docs = rag_query(query, retriever, coll) or []\n",
    "    if not docs:\n",
    "        return \"No market context found for this query.\"\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "market_tool = Tool(\n",
    "    name=\"MarketData\",\n",
    "    func=market_tool_func,\n",
    "    description=(\n",
    "        \"Returns market-related context from the internal RAG setup. \"\n",
    "        \"Input can be a stock ticker (AAPL, TSLA) or a company name (Apple, Tesla).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Comparison tool\n",
    "def compare_tool_func(query: str):\n",
    "    \"\"\"\n",
    "    Compare two stocks given as 'AAPL,MSFT' or text like 'Compare Apple and Microsoft'.\n",
    "    \"\"\"\n",
    "    query = query.strip()\n",
    "\n",
    "    # Try comma-separated first\n",
    "    if \",\" in query:\n",
    "        parts = [x.strip() for x in query.split(\",\") if x.strip()]\n",
    "        if len(parts) != 2:\n",
    "            return \"Please provide exactly two stocks, e.g. 'AAPL,MSFT'.\"\n",
    "        stock1, stock2 = parts\n",
    "    else:\n",
    "        # Fallback: split on 'and'\n",
    "        parts = re.split(r\"\\band\\b\", query, flags=re.IGNORECASE)\n",
    "        if len(parts) == 2:\n",
    "            stock1, stock2 = parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            return \"Please provide two stocks, e.g. 'AAPL,MSFT'.\"\n",
    "\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "    docs1 = rag_query(stock1, retriever, coll) or []\n",
    "    docs2 = rag_query(stock2, retriever, coll) or []\n",
    "\n",
    "    summary1 = \"\\n\".join(d.page_content for d in docs1) or \"No context found.\"\n",
    "    summary2 = \"\\n\".join(d.page_content for d in docs2) or \"No context found.\"\n",
    "\n",
    "    return (\n",
    "        f\"Stock 1: {stock1}\\n{summary1}\\n\\n\"\n",
    "        f\"Stock 2: {stock2}\\n{summary2}\\n\\n\"\n",
    "        \"Now provide pros/cons and a final verdict.\"\n",
    "    )\n",
    "\n",
    "\n",
    "compare_tool = Tool(\n",
    "    name=\"CompareStocks\",\n",
    "    func=compare_tool_func,\n",
    "    description=\"Compare two stocks. Input: 'AAPL,MSFT' or 'Compare Apple and Microsoft'.\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_agent():\n",
    "    \"\"\"\n",
    "    Create a LangChain agent wired up with risk, market, and comparison tools.\n",
    "    \"\"\"\n",
    "    if not OPENAI_KEY:\n",
    "        raise ValueError(\"Missing OPENAI_API_KEY.\")\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.2,\n",
    "        openai_api_key=OPENAI_KEY,\n",
    "    )\n",
    "\n",
    "    tools = [risk_tool, market_tool, compare_tool]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "    )\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ad82b9a-4c83-407f-96f0-8f4301a12e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahil\\AppData\\Local\\Temp\\ipykernel_13068\\2243317609.py:26: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm_client = ChatOpenAI(\n",
      "C:\\Users\\rahil\\AppData\\Local\\Temp\\ipykernel_13068\\2243317609.py:32: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  profile_parser = LLMChain(llm=llm_client, prompt=profile_prompt)\n"
     ]
    }
   ],
   "source": [
    "# Block 8.1 - Prompt + LLM chain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "# Ensure the key exists (fallback to env directly)\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "profile_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "You're a helpful financial assistant. Read the user's profile text and return a JSON object with these keys:\n",
    "- Age Group: one of \"18-25\", \"26-35\", \"36-45\", \"46-60\", \"60+\"\n",
    "- Income Group: one of \"<30,000\", \"30,000-70,000\", \"70,000+\"\n",
    "- Employment Status: e.g. \"Salaried\", \"Self-employed\", \"Retired\"\n",
    "- Loan Status: one of \"approved\", \"pending\", \"rejected\"\n",
    "- Investment Goal: e.g. \"Growth\", \"Wealth Preservation\", \"Short-term Safety\"\n",
    "- Investment Amount: numeric (INR)\n",
    "\n",
    "User text: {text}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm_client = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=OPENAI_KEY\n",
    ")\n",
    "\n",
    "profile_parser = LLMChain(llm=llm_client, prompt=profile_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0895cb48-939c-453e-af07-605d1f4e4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 - Allocation and projection utilities\n",
    "def allocation_for_risk(risk: str, goal: str):\n",
    "    if risk == \"High\":\n",
    "        return {\"Stocks\": 0.70, \"FD\": 0.20, \"Gold\": 0.10}\n",
    "    if risk == \"Medium\":\n",
    "        return {\"Stocks\": 0.50, \"FD\": 0.30, \"Gold\": 0.20}\n",
    "    return {\"Stocks\": 0.30, \"FD\": 0.40, \"Gold\": 0.30}\n",
    "\n",
    "def simulate_growth(principal: float, split: dict, years: int = 5):\n",
    "    rates = {\"Stocks\": 0.12, \"FD\": 0.06, \"Gold\": 0.08}\n",
    "    portfolio_vals = [principal]\n",
    "    for _ in range(years):\n",
    "        prev = portfolio_vals[-1]\n",
    "        next_total = sum(prev * split[k] * (1 + rates[k]) for k in split)\n",
    "        portfolio_vals.append(next_total)\n",
    "    fd_series = [principal * ((1 + rates[\"FD\"])**i) for i in range(years+1)]\n",
    "    gold_series = [principal * ((1 + rates[\"Gold\"])**i) for i in range(years+1)]\n",
    "    return portfolio_vals, fd_series, gold_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb0d19e-e406-4eef-b76d-73469039ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 - Fetch history + basic indicators (with NSE .NS fallback)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_history(ticker: str, period: str = \"6mo\", interval: str = \"1d\"):\n",
    "    \"\"\"\n",
    "    Fetch price history for a symbol.\n",
    "\n",
    "    - First tries the raw symbol (e.g. INFY, AAPL).\n",
    "    - If that returns no data and the symbol has no suffix,\n",
    "      also tries '<SYMBOL>.NS' for common Indian stocks (TCS, RELIANCE, etc.).\n",
    "    \"\"\"\n",
    "    base = (ticker or \"\").strip()\n",
    "    if not base:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    candidates = [base]\n",
    "    if \".\" not in base:  # no exchange suffix provided -> also try NSE\n",
    "        candidates.append(base.upper() + \".NS\")\n",
    "\n",
    "    for sym in candidates:\n",
    "        try:\n",
    "            df = yf.Ticker(sym).history(period=period, interval=interval)\n",
    "            if df is not None and not df.empty:\n",
    "                df = df.reset_index()\n",
    "\n",
    "                # Ensure there is a 'Date' column for plotting\n",
    "                if \"Date\" not in df.columns:\n",
    "                    # yfinance usually names the index 'Date', but just in case:\n",
    "                    df.rename(columns={df.columns[0]: \"Date\"}, inplace=True)\n",
    "\n",
    "                # Optional: keep track of which symbol actually worked\n",
    "                df[\"__symbol__\"] = sym\n",
    "                return df\n",
    "        except Exception as exc:\n",
    "            print(f\"[get_history] yfinance error for {sym}: {exc}\")\n",
    "\n",
    "    # If nothing worked, return empty frame\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def add_indicators(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Add SMA20, EMA20, RSI14, MACD, MACD_Signal columns to a price DataFrame.\n",
    "    Expects a 'Close' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Close\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    # SMA / EMA\n",
    "    df[\"SMA20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\n",
    "    df[\"EMA20\"] = df[\"Close\"].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "    # RSI14 (standard EMA-based)\n",
    "    diff = df[\"Close\"].diff()\n",
    "    up = diff.clip(lower=0)\n",
    "    down = -diff.clip(upper=0)\n",
    "    ma_up = up.ewm(com=13, adjust=False).mean()\n",
    "    ma_down = down.ewm(com=13, adjust=False).mean()\n",
    "    rs = ma_up / (ma_down + 1e-9)\n",
    "    df[\"RSI14\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = ema12 - ema26\n",
    "    df[\"MACD_Signal\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_with_indicators(df: pd.DataFrame, ticker: str):\n",
    "    \"\"\"\n",
    "    Plot price + SMA20/EMA20, RSI14, and MACD for a given history DataFrame.\n",
    "    Returns a matplotlib Figure (or None if df is empty).\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or \"Close\" not in df.columns or \"Date\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    df2 = add_indicators(df)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(8, 8))\n",
    "    gs = fig.add_gridspec(3, 1, height_ratios=[3, 1, 1])\n",
    "\n",
    "    # Price + MAs\n",
    "    ax_price = fig.add_subplot(gs[0, 0])\n",
    "    ax_price.plot(df2[\"Date\"], df2[\"Close\"], lw=1, label=\"Close\")\n",
    "    ax_price.plot(df2[\"Date\"], df2[\"SMA20\"], lw=1, label=\"SMA20\")\n",
    "    ax_price.plot(df2[\"Date\"], df2[\"EMA20\"], lw=1, label=\"EMA20\")\n",
    "    ax_price.set_title(f\"{ticker} — Price with SMA/EMA\")\n",
    "    ax_price.legend(loc=\"upper left\")\n",
    "\n",
    "    # RSI\n",
    "    ax_rsi = fig.add_subplot(gs[1, 0], sharex=ax_price)\n",
    "    ax_rsi.plot(df2[\"Date\"], df2[\"RSI14\"], label=\"RSI14\")\n",
    "    ax_rsi.axhline(70, linestyle=\"--\", linewidth=0.7)\n",
    "    ax_rsi.axhline(30, linestyle=\"--\", linewidth=0.7)\n",
    "    ax_rsi.set_ylabel(\"RSI\")\n",
    "\n",
    "    # MACD\n",
    "    ax_macd = fig.add_subplot(gs[2, 0], sharex=ax_price)\n",
    "    ax_macd.plot(df2[\"Date\"], df2[\"MACD\"], label=\"MACD\")\n",
    "    ax_macd.plot(df2[\"Date\"], df2[\"MACD_Signal\"], label=\"Signal\")\n",
    "    ax_macd.set_ylabel(\"MACD\")\n",
    "    ax_macd.legend(loc=\"upper left\")\n",
    "\n",
    "    for lbl in ax_macd.get_xticklabels():\n",
    "        lbl.set_rotation(30)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8f5324e-5681-4441-a4fb-2eccb1e60b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 - Risk profiler runner\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_risk_profile(user_text: str,\n",
    "                     age_sel, income_sel, employment_sel,\n",
    "                     loan_sel, goal_sel, invest_amount):\n",
    "    # initial status\n",
    "    yield (\"Parsing / running...\", \"\", None, None)\n",
    "\n",
    "    parsed = None\n",
    "    parse_note = None\n",
    "\n",
    "    # 1) Try free-text LLM parser\n",
    "    if user_text and user_text.strip():\n",
    "        try:\n",
    "            raw = profile_parser.run({\"text\": user_text})\n",
    "            parsed = json.loads(raw)\n",
    "        except Exception as exc:\n",
    "            parsed = None\n",
    "            parse_note = f\"Parsing failed: {exc} — using form values.\"\n",
    "\n",
    "    # 2) Start from form values\n",
    "    age_group = age_sel\n",
    "    income_group = income_sel\n",
    "    employment = employment_sel\n",
    "    loan_status = loan_sel\n",
    "    goal = goal_sel\n",
    "    try:\n",
    "        amount = float(invest_amount or 0)\n",
    "    except Exception:\n",
    "        amount = 0.0\n",
    "\n",
    "    # 3) If parsing worked, override with parsed fields (LLM keys → model keys)\n",
    "    if isinstance(parsed, dict):\n",
    "        age_group = parsed.get(\"Age Group\", age_group)\n",
    "        income_group = parsed.get(\"Income Group\", income_group)\n",
    "        employment = parsed.get(\"Employment Status\", employment)\n",
    "        loan_status = parsed.get(\"Loan Status\", loan_status)\n",
    "        goal = parsed.get(\"Investment Goal\", goal)\n",
    "        try:\n",
    "            amount = float(parsed.get(\"Investment Amount\", amount) or 0)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Normalise loan status to approved / pending / rejected\n",
    "    loan_str = str(loan_status).strip().lower()\n",
    "    if \"approve\" in loan_str:\n",
    "        loan_norm = \"approved\"\n",
    "    elif \"pend\" in loan_str:\n",
    "        loan_norm = \"pending\"\n",
    "    elif \"reject\" in loan_str:\n",
    "        loan_norm = \"rejected\"\n",
    "    else:\n",
    "        # fall back to dropdown value (should already be one of the three)\n",
    "        loan_norm = str(loan_sel)\n",
    "\n",
    "    # 5) Build feature dict exactly as the XGBoost pipeline expects\n",
    "    features = {\n",
    "        \"AgeGroup\": age_group,\n",
    "        \"IncomeGroup\": income_group,\n",
    "        \"EmploymentStatus\": employment,\n",
    "        \"InvestmentGoal\": goal,\n",
    "        \"LoanStatus\": loan_norm,\n",
    "        \"InvestmentAmount\": amount,\n",
    "    }\n",
    "\n",
    "    # 6) Model prediction\n",
    "    try:\n",
    "        model_bundle = load_xgb_model()\n",
    "        pred = predict_risk(model_bundle, features)\n",
    "    except Exception as exc:\n",
    "        yield (\"\", f\"Prediction failed: {exc}\", None, None)\n",
    "        return\n",
    "\n",
    "    risk_label = pred.get(\"prediction\", \"Medium\")\n",
    "    confidence = pred.get(\"probability\", 0.6)\n",
    "\n",
    "    # 7) Allocation + plots\n",
    "    split = allocation_for_risk(risk_label, goal)\n",
    "\n",
    "    # pie chart\n",
    "    fig1, ax1 = plt.subplots(figsize=(4, 3))\n",
    "    ax1.pie(split.values(), labels=split.keys(), autopct=\"%1.1f%%\", startangle=90)\n",
    "    ax1.set_title(\"Allocation\")\n",
    "\n",
    "    # 5-year projection\n",
    "    port, fd_s, gold_s = simulate_growth(amount, split, years=5)\n",
    "    yrs = np.arange(0, 6)\n",
    "    fig2, ax2 = plt.subplots(figsize=(6, 3))\n",
    "    ax2.plot(yrs, port, \"o-\", label=\"Portfolio\")\n",
    "    ax2.plot(yrs, fd_s, \"s--\", label=\"FD-only\")\n",
    "    ax2.plot(yrs, gold_s, \"d--\", label=\"Gold-only\")\n",
    "    ax2.set_title(\"5-year projection\")\n",
    "    ax2.set_xlabel(\"Year\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "    ax2.legend()\n",
    "\n",
    "    # 8) Markdown output\n",
    "    text = f\"### Risk: **{risk_label}**\\n\\nConfidence: {confidence:.2%}\\n\\nAllocation: \"\n",
    "    text += \", \".join(f\"{k}: {v*100:.0f}%\" for k, v in split.items())\n",
    "    if parse_note:\n",
    "        text = f\"**Note:** {parse_note}\\n\\n\" + text\n",
    "\n",
    "    yield (\"\", text, fig1, fig2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59f2f959-1646-44a8-9602-898194a1440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 - Stock analysis runner (filtered RAG per ticker)\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "explain_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=OPENAI_KEY,\n",
    ")\n",
    "\n",
    "explain_prompt = PromptTemplate(\n",
    "    input_variables=[\"ticker\", \"trend\", \"rsi\", \"macd\"],\n",
    "    template=\"\"\"\n",
    "You're writing a short, plain-language explanation for an investor.\n",
    "\n",
    "Ticker: {ticker}\n",
    "Trend (one line): {trend}\n",
    "RSI note: {rsi}\n",
    "MACD note: {macd}\n",
    "\n",
    "Write a 2-3 sentence explanation.\n",
    "\"\"\",\n",
    ")\n",
    "explain_chain = LLMChain(llm=explain_llm, prompt=explain_prompt)\n",
    "\n",
    "\n",
    "def _filter_docs_for_ticker(docs, ticker: str):\n",
    "    \"\"\"\n",
    "    Keep only documents that look relevant to the given ticker:\n",
    "    - metadata['ticker'] == ticker (if present), OR\n",
    "    - the text itself mentions the ticker.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    t = ticker.strip().upper()\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        text = (getattr(d, \"page_content\", \"\") or \"\")\n",
    "        meta = getattr(d, \"metadata\", {}) or {}\n",
    "        meta_ticker = str(meta.get(\"ticker\", \"\")).upper()\n",
    "        if not t:\n",
    "            out.append(d)\n",
    "            continue\n",
    "\n",
    "        if meta_ticker == t or t in text.upper():\n",
    "            out.append(d)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _clean_research_snip(docs, ticker: str, max_chars: int = 1200) -> str:\n",
    "    \"\"\"\n",
    "    Build a short research snippet:\n",
    "    - Only use docs relevant to the ticker.\n",
    "    - Inside each doc, keep only sentences that mention the ticker.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"No research available.\"\n",
    "\n",
    "    t = ticker.strip().upper()\n",
    "    snippets = []\n",
    "\n",
    "    for d in docs:\n",
    "        text = (getattr(d, \"page_content\", \"\") or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if t:\n",
    "            # split into sentences, keep those mentioning the ticker\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "            hit_sents = [s for s in sentences if t in s.upper()]\n",
    "            text = \" \".join(hit_sents).strip()\n",
    "\n",
    "        if text:\n",
    "            snippets.append(text)\n",
    "\n",
    "    if not snippets:\n",
    "        return \"No research available.\"\n",
    "\n",
    "    joined = \"\\n\\n\".join(snippets).strip()\n",
    "    return joined[:max_chars] if len(joined) > max_chars else joined\n",
    "\n",
    "\n",
    "def run_stock_analysis_enhanced(query: str):\n",
    "    yield (\"Fetching chart...\", \"\", None)\n",
    "\n",
    "    ticker = query.strip()\n",
    "\n",
    "    # --- RAG / research text (filtered) ---\n",
    "    try:\n",
    "        client, coll = init_chroma()\n",
    "        retriever = make_retriever(4)\n",
    "        docs_raw = rag_query(ticker, retriever, coll)\n",
    "        docs_filt = _filter_docs_for_ticker(docs_raw, ticker)\n",
    "        rag_snip = _clean_research_snip(docs_filt, ticker)\n",
    "    except Exception as exc:\n",
    "        print(\"[run_stock_analysis_enhanced] RAG error:\", exc)\n",
    "        rag_snip = \"No research available.\"\n",
    "\n",
    "    # --- Price history + indicators ---\n",
    "    hist = get_history(ticker, period=\"6mo\")\n",
    "    fig = plot_with_indicators(hist, ticker) if not hist.empty else None\n",
    "\n",
    "    trend = \"insufficient data\"\n",
    "    rsi_note = \"RSI unavailable\"\n",
    "    macd_note = \"MACD unavailable\"\n",
    "\n",
    "    try:\n",
    "        if not hist.empty:\n",
    "            dfi = add_indicators(hist)\n",
    "            change = (dfi[\"Close\"].iloc[-1] - dfi[\"Close\"].iloc[0]) / (dfi[\"Close\"].iloc[0] + 1e-9)\n",
    "            trend = \"Uptrend\" if change > 0.03 else (\"Downtrend\" if change < -0.03 else \"Sideways\")\n",
    "\n",
    "            last_rsi = dfi[\"RSI14\"].iloc[-1]\n",
    "            rsi_note = (\n",
    "                f\"RSI ~{last_rsi:.1f} — \"\n",
    "                + (\"Overbought\" if last_rsi > 70 else (\"Oversold\" if last_rsi < 30 else \"Neutral\"))\n",
    "            )\n",
    "\n",
    "            m_val = dfi[\"MACD\"].iloc[-1]\n",
    "            s_val = dfi[\"MACD_Signal\"].iloc[-1]\n",
    "            macd_note = (\n",
    "                f\"MACD {m_val:.3f} vs {s_val:.3f} — \"\n",
    "                + (\"Bullish\" if m_val > s_val else \"Bearish/Neutral\")\n",
    "            )\n",
    "    except Exception as exc:\n",
    "        print(\"[run_stock_analysis_enhanced] indicator error:\", exc)\n",
    "\n",
    "    # --- Natural-language explanation ---\n",
    "    try:\n",
    "        expl = explain_chain.run(\n",
    "            {\"ticker\": ticker, \"trend\": trend, \"rsi\": rsi_note, \"macd\": macd_note}\n",
    "        )\n",
    "    except Exception:\n",
    "        expl = f\"{trend}. {rsi_note}. {macd_note}.\"\n",
    "\n",
    "    md = (\n",
    "        f\"**Research:**\\n\\n{rag_snip}\\n\\n\"\n",
    "        f\"**Chart note:**\\n\\n{expl}\\n\\n\"\n",
    "        f\"**Tech details:**\\n- Trend: {trend}\\n- {rsi_note}\\n- {macd_note}\"\n",
    "    )\n",
    "\n",
    "    yield (\"\", md, fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adb8061a-cfeb-4d8d-995e-077654d980c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Block 8.6 — Compare two tickers (updated, filtered RAG + robust metrics)\n",
    "# -----------------------------\n",
    "import re\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# safe LLM chain for pros/cons (uses OPENAI_KEY from your env)\n",
    "proscons_prompt = PromptTemplate(\n",
    "    input_variables=[\"a\", \"b\", \"obs\"],\n",
    "    template=\"\"\"\n",
    "You are a concise, pragmatic equity analyst.\n",
    "\n",
    "Observations:\n",
    "{obs}\n",
    "\n",
    "Provide 2-4 short pros for {a} over {b}, 2-4 short cons, and a one-line verdict for a conservative investor.\n",
    "Use plain, non-technical language and keep each bullet short.\n",
    "\"\"\",\n",
    ")\n",
    "proscons_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, openai_api_key=globals().get(\"OPENAI_KEY\"))\n",
    "proscons_chain = LLMChain(llm=proscons_llm, prompt=proscons_prompt)\n",
    "\n",
    "\n",
    "def _metrics_from_history(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Return structured metrics from price history DataFrame (expects 'Close' column).\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or \"Close\" not in df.columns:\n",
    "        return {\"ok\": False}\n",
    "    s = df[\"Close\"].dropna().astype(float)\n",
    "    if s.empty:\n",
    "        return {\"ok\": False}\n",
    "\n",
    "    n = len(s)\n",
    "    last_price = float(s.iloc[-1])\n",
    "\n",
    "    one_y = None\n",
    "    if n >= 252:\n",
    "        base_idx = max(0, n - 252)\n",
    "        base = float(s.iloc[base_idx])\n",
    "        if base != 0:\n",
    "            one_y = (last_price - base) / base\n",
    "\n",
    "    three_y = None\n",
    "    if n > 1:\n",
    "        base = float(s.iloc[0])\n",
    "        if base != 0:\n",
    "            three_y = (last_price - base) / base\n",
    "\n",
    "    daily = s.pct_change().dropna()\n",
    "    vol = float(daily.std() * (252 ** 0.5)) if not daily.empty else None\n",
    "    rf = 0.04\n",
    "    sharpe_like = None\n",
    "    if vol is not None and vol > 0 and one_y is not None:\n",
    "        sharpe_like = (one_y - rf) / vol\n",
    "\n",
    "    return {\"ok\": True, \"last\": last_price, \"1y\": one_y, \"3y\": three_y, \"vol\": vol, \"sharpe\": sharpe_like}\n",
    "\n",
    "\n",
    "def _fmt_snapshot(ticker: str, m: dict) -> str:\n",
    "    \"\"\"\n",
    "    Nicely format the metric snapshot for UI markdown.\n",
    "    \"\"\"\n",
    "    if not m.get(\"ok\"):\n",
    "        return f\"### {ticker} — No price history available\"\n",
    "    lines = [f\"### {ticker} — Price summary\", f\"- Latest: ₹{m['last']:.2f}\"]\n",
    "    if m.get(\"1y\") is not None:\n",
    "        lines.append(f\"- 1y: {m['1y']*100:.2f}%\")\n",
    "    if m.get(\"3y\") is not None:\n",
    "        lines.append(f\"- 3y: {m['3y']*100:.2f}%\")\n",
    "    if m.get(\"vol\") is not None:\n",
    "        lines.append(f\"- Annualised vol (approx): {m['vol']*100:.2f}%\")\n",
    "    if m.get(\"sharpe\") is not None:\n",
    "        lines.append(f\"- Sharpe-like: {m['sharpe']:.2f}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _doc_about_ticker(doc, ticker: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: is this Document clearly about 'ticker'?\n",
    "    - Accept if metadata.ticker matches\n",
    "    - Or if page_content contains whole-word ticker (case-insensitive)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        meta = getattr(doc, \"metadata\", {}) or {}\n",
    "        if isinstance(meta, dict):\n",
    "            t = (meta.get(\"ticker\") or meta.get(\"Ticker\") or meta.get(\"symbol\"))\n",
    "            if isinstance(t, str) and t.strip().upper() == ticker.upper():\n",
    "                return True\n",
    "        text = (getattr(doc, \"page_content\", \"\") or \"\")\n",
    "        if re.search(rf\"\\b{re.escape(ticker)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def _try_yf_history(ticker: str, period: str = \"3y\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robustly download ticker history: try ticker, and for Indian tickers also try ticker + '.NS'.\n",
    "    Returns DataFrame (can be empty).\n",
    "    \"\"\"\n",
    "    t = (ticker or \"\").strip().upper()\n",
    "    candidates = [t]\n",
    "    # if short or looks local, attempt .NS fallback (keeps previous behavior but safe)\n",
    "    if not t.endswith(\".NS\") and len(t) <= 6:\n",
    "        candidates.append(t + \".NS\")\n",
    "\n",
    "    for tk in candidates:\n",
    "        try:\n",
    "            df = yf.download(tk, period=period, progress=False)\n",
    "            if df is None or df.empty:\n",
    "                # try fallback .history (sometimes works)\n",
    "                df = yf.Ticker(tk).history(period=period)\n",
    "            if df is not None and not df.empty:\n",
    "                return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def run_compare_generator(sym1: str, sym2: str):\n",
    "    \"\"\"\n",
    "    Generator for Gradio UI:\n",
    "      - yields (\"status\", \"message\") followed by (\"\", final_markdown)\n",
    "    Output (final) is a single markdown string containing:\n",
    "      - Price snapshots\n",
    "      - Price-based comparison lines\n",
    "      - Pros/Cons + verdict (LLM or heuristic)\n",
    "      - Filtered RAG / research snippets placed after the main comparison (to avoid noise)\n",
    "    \"\"\"\n",
    "    s1 = (sym1 or \"\").strip().upper()\n",
    "    s2 = (sym2 or \"\").strip().upper()\n",
    "    yield (\"Gathering data...\", \"\")\n",
    "\n",
    "    if not s1 or not s2:\n",
    "        yield (\"\", \"Please provide two tickers.\")\n",
    "        return\n",
    "\n",
    "    # 1) Download price histories (robust)\n",
    "    h1 = _try_yf_history(s1, period=\"3y\")\n",
    "    h2 = _try_yf_history(s2, period=\"3y\")\n",
    "\n",
    "    m1 = _metrics_from_history(h1)\n",
    "    m2 = _metrics_from_history(h2)\n",
    "\n",
    "    parts = []\n",
    "    parts.append(_fmt_snapshot(s1, m1))\n",
    "    parts.append(\"\\n\\n\")\n",
    "    parts.append(_fmt_snapshot(s2, m2))\n",
    "    parts.append(\"\\n\\n\")\n",
    "\n",
    "    # 2) Price-based comparison\n",
    "    if m1.get(\"ok\") and m2.get(\"ok\"):\n",
    "        cmp_lines = [\"### Price-based metrics (approx)\"]\n",
    "        cmp_lines.append(f\"- {s1} 1y: {m1.get('1y', 'N/A')*100:.2f}%\" if m1.get(\"1y\") is not None else f\"- {s1} 1y: N/A\")\n",
    "        cmp_lines.append(f\"- {s2} 1y: {m2.get('1y', 'N/A')*100:.2f}%\" if m2.get(\"1y\") is not None else f\"- {s2} 1y: N/A\")\n",
    "        cmp_lines.append(f\"- {s1} vol: {m1.get('vol', 0)*100:.2f}%\" if m1.get(\"vol\") is not None else f\"- {s1} vol: N/A\")\n",
    "        cmp_lines.append(f\"- {s2} vol: {m2.get('vol', 0)*100:.2f}%\" if m2.get(\"vol\") is not None else f\"- {s2} vol: N/A\")\n",
    "        parts.append(\"\\n\".join(cmp_lines))\n",
    "\n",
    "        # Try LLM pros/cons; safe fallback if LLM fails\n",
    "        try:\n",
    "            obs = \"\\n\".join(cmp_lines)\n",
    "            pc = proscons_chain.run({\"a\": s1, \"b\": s2, \"obs\": obs})\n",
    "            parts.append(\"\\n\\n### Pros/Cons & Verdict\\n\")\n",
    "            parts.append(pc.strip())\n",
    "        except Exception:\n",
    "            # Heuristic fallback: compare sharpe-like minus vol\n",
    "            score1 = (m1.get(\"sharpe\") or 0) - (m1.get(\"vol\") or 0)\n",
    "            score2 = (m2.get(\"sharpe\") or 0) - (m2.get(\"vol\") or 0)\n",
    "            if score1 > score2:\n",
    "                parts.append(\"\\n\\nFinal verdict: {} looks preferable for a conservative investor (higher risk-adjusted).\".format(s1))\n",
    "            else:\n",
    "                parts.append(\"\\n\\nFinal verdict: {} looks preferable for a conservative investor (higher risk-adjusted).\".format(s2))\n",
    "    else:\n",
    "        parts.append(\"**Price-based comparison not available for one or both tickers.**\\n\\n\")\n",
    "\n",
    "    # 3) RAG retrieval (filtered) — placed AFTER comparison to avoid noise at top\n",
    "    try:\n",
    "        client, coll = init_chroma()\n",
    "        retriever = make_retriever()\n",
    "        raw_docs1 = rag_query(s1, retriever, coll) or []\n",
    "        raw_docs2 = rag_query(s2, retriever, coll) or []\n",
    "    except Exception:\n",
    "        raw_docs1 = raw_docs2 = []\n",
    "\n",
    "    # Keep only docs that clearly mention the ticker\n",
    "    rel1 = [d for d in raw_docs1 if _doc_about_ticker(d, s1)]\n",
    "    rel2 = [d for d in raw_docs2 if _doc_about_ticker(d, s2)]\n",
    "\n",
    "    # If no RAG docs relevant, try yfinance short summary\n",
    "    if not rel1:\n",
    "        sd1 = fetch_stock_data(s1)\n",
    "        if sd1:\n",
    "            rel1 = [sd1]\n",
    "    if not rel2:\n",
    "        sd2 = fetch_stock_data(s2)\n",
    "        if sd2:\n",
    "            rel2 = [sd2]\n",
    "\n",
    "    if rel1:\n",
    "        parts.append(f\"\\n\\n### RAG / Research — {s1}\\n\")\n",
    "        for d in rel1:\n",
    "            parts.append(f\"- { (d.page_content or '').strip() }\\n\")\n",
    "    if rel2:\n",
    "        parts.append(f\"\\n\\n### RAG / Research — {s2}\\n\")\n",
    "        for d in rel2:\n",
    "            parts.append(f\"- { (d.page_content or '').strip() }\\n\")\n",
    "\n",
    "    final_md = \"\\n\".join(parts)\n",
    "    yield (\"\", final_md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2ee552e-1215-4e6b-9a92-d192867669bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 rows and 18 columns\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8.7 - UI layout and launch\n",
    "import gradio as gr\n",
    "\n",
    "# try loading dataset (optional, for dropdown auto-fill)\n",
    "try:\n",
    "    df_opts = load_data()\n",
    "except Exception:\n",
    "    df_opts = None\n",
    "\n",
    "def get_choices(col, default):\n",
    "    if df_opts is None or col not in (df_opts.columns if hasattr(df_opts, \"columns\") else []):\n",
    "        return default\n",
    "    vals = sorted([str(x) for x in df_opts[col].dropna().unique().tolist()], key=str.lower)\n",
    "    return vals if vals else default\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Fixed dropdown options\n",
    "# -----------------------------\n",
    "age_opts = [\"18-25\", \"26-35\", \"36-45\", \"46-60\", \"60+\"]\n",
    "income_opts = [\"<30,000\", \"30,000-70,000\", \"70,000+\"]\n",
    "\n",
    "employment_opts = get_choices(\n",
    "    \"Employment Status\",\n",
    "    [\"Salaried\", \"Self-employed\", \"Retired\"]\n",
    ")\n",
    "\n",
    "#  Now using approved / pending / rejected\n",
    "loan_opts = [\"approved\", \"pending\", \"rejected\"]\n",
    "\n",
    "goal_opts = get_choices(\n",
    "    \"Investment Goals\",\n",
    "    [\"Growth\", \"Wealth Preservation\", \"Short-term Safety\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Gradio App\n",
    "\n",
    "with gr.Blocks(title=\"FinRagAssist\") as demo:\n",
    "    gr.Markdown(\"# **FinRagAssist — Smart Investment Advisor**\")\n",
    "\n",
    "    # RISK PROFILER TAB\n",
    "    with gr.Tab(\"Risk Profiler\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                free_text = gr.Textbox(\n",
    "                    label=\"Optional: profile text\",\n",
    "                    placeholder=\"e.g. I'm 32, salaried, 80k/month, loan approved, goal growth, invest 50k\"\n",
    "                )\n",
    "                age_in = gr.Dropdown(label=\"Age Group\", choices=age_opts, value=\"26-35\")\n",
    "                income_in = gr.Dropdown(label=\"Income Group\", choices=income_opts, value=\"30,000-70,000\")\n",
    "                emp_in = gr.Dropdown(label=\"Employment Status\", choices=employment_opts, value=employment_opts[0])\n",
    "            \n",
    "            with gr.Column():\n",
    "                loan_in = gr.Dropdown(label=\"Loan Status\", choices=loan_opts, value=\"approved\")\n",
    "                goal_in = gr.Dropdown(label=\"Investment Goal\", choices=goal_opts, value=goal_opts[0])\n",
    "                amt_in = gr.Number(label=\"Investment Amount (₹)\", value=25000)\n",
    "\n",
    "                btn = gr.Button(\"Get Recommendation\")\n",
    "\n",
    "                status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                out_md = gr.Markdown()\n",
    "                pie = gr.Plot()\n",
    "                growth = gr.Plot()\n",
    "\n",
    "        btn.click(\n",
    "            run_risk_profile,\n",
    "            inputs=[free_text, age_in, income_in, emp_in, loan_in, goal_in, amt_in],\n",
    "            outputs=[status_box, out_md, pie, growth]\n",
    "        )\n",
    "\n",
    "    # STOCK ANALYSIS TAB\n",
    "    with gr.Tab(\"Stock Analysis\"):\n",
    "        ticker = gr.Textbox(label=\"Ticker or Name\")\n",
    "        analyze_btn = gr.Button(\"Analyze\")\n",
    "\n",
    "        st_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "        st_md = gr.Markdown()\n",
    "        st_fig = gr.Plot()\n",
    "\n",
    "        analyze_btn.click(\n",
    "            run_stock_analysis_enhanced,\n",
    "            inputs=[ticker],\n",
    "            outputs=[st_status, st_md, st_fig]\n",
    "        )\n",
    "\n",
    "    # COMPARE TAB\n",
    "    with gr.Tab(\"Compare Stocks\"):\n",
    "        a = gr.Textbox(label=\"Ticker 1\")\n",
    "        b = gr.Textbox(label=\"Ticker 2\")\n",
    "        cmp_btn = gr.Button(\"Compare\")\n",
    "\n",
    "        cmp_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "        cmp_out = gr.Markdown()\n",
    "\n",
    "        cmp_btn.click(\n",
    "            run_compare_generator,\n",
    "            inputs=[a, b],\n",
    "            outputs=[cmp_status, cmp_out]\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb65c8-0aeb-4467-84e0-6f2059361333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32895fe-454d-41da-b8ee-df917399d28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5572ad-28b5-42d9-94f0-6c773abda849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913da680-4e41-4eb3-bc8d-7cf820efb850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e926504-a9c1-41a0-ac50-f1a2b173750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32625825-02bf-42ab-99d6-e09ac704e8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
